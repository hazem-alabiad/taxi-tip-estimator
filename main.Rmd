---
title: "cmp713-project"
author: H. White & B. Bayrak
date: 25/01/20213
output: rmarkdown::html_vignette
---
# Loading Libraries
```{r}
library(randomForest)
library(ggmap)
library(h2o)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(nnet)
library(hrbrthemes)
library(rpart.plot)
library(mlbench)
library(e1071)
library(rpart)
library(DMwR2)
library(caret)
library(Metrics)
library(fpc)
library(Hmisc)
library(tibble)
library(knitr)
library(earth) # fit MARS models
```
# Custom Functions
```{r}
`%notin%` <- Negate(`%in%`)

pretty_df_print <- function (df) kable_styling(kable(df))

top_least_freq_and_neg_vals <- function (col) {
  t <- table(col) %>% as.data.frame() %>% arrange(desc(Freq))
  non_pos <- as.data.frame(col)
  non_pos <- non_pos[non_pos <=0, ]
  print("Top frequent values:")
  print.data.frame(head(t))
  print("Least frequent values:")
  print.data.frame(tail(t))
  print("Negative values:")
  print.data.frame(non_pos)
  print("/////////////////////////////////////////////////////")
}

na_count_df <- function (df) {
  return(as.data.frame(t(data.frame(sapply(df, function(col) sum(is.na(col)))))))
}

iqr_outliers_min_max <- function(df) {
  iqr_df <- as.data.frame( t(data.frame(df %>% sapply(function (x) {quantile(x, c(0.25, 0.75))}))) ) %>% cbind((data.frame(df %>% sapply(IQR))))
  colnames(iqr_df) <- c("Q1", "Q3", "IQR")
  iqr_df$minVal <- iqr_df$Q1 - 1.5*iqr_df$IQR
  iqr_df$maxVal <- iqr_df$Q3 + 1.5*iqr_df$IQR
  print.data.frame(iqr_df %>% select("minVal", "maxVal"))
}

iqr_outliers_finder <- function(x){
  Q1 <- quantile(x, 0.25)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  Vl <- Q1 - 1.5 * IQR
  Vr <- Q3 + 1.5 * IQR
  return (x[which(x < Vl | x > Vr)])
}

get_freq_df_from_vector <- function (vec, col.name) {
  freq_df <- as.data.frame(table(vec))
  colnames(freq_df)[1] <- col.name
  freq_df <- freq_df[order(freq_df[,1], decreasing = T),]
  if(nrow(freq_df) <= 10) {
    print.data.frame(freq_df)}
  else {
    cat("Highest values :\n")
    print.data.frame(head(freq_df))
    cat("Lowest values :\n")
    print.data.frame(tail(freq_df))
  }
}

dbscan_outliers_finder <- function(data, ...) {
  require(fpc, quietly=TRUE)
  cl <- dbscan(data, ...)
  posOuts <- which(cl$cluster == 0)
  list(positions = posOuts,
       outliers = data[posOuts,],
       dbscanResults = cl)
}
```
# Data loading
```{r}
st <- proc.time()
green <- read.csv("data/green_tripdata_2020-06.csv")
green <- green %>% rbind(read.csv("data/green_tripdata_2020-01.csv"))
print(proc.time() - st)
cat('There are [', nrow(green), '] observations, and [',ncol(green),'] variables')
```


# Data exploratory & Preprocessing
## Dataframe head
```{r}
head(green)
```
## Dataframe tail
```{r}
tail(green)
```
## Brief info on the data
```{r}
str(green)
```
## Unnecessary Variables Deletion
```{r}
num.vars <- ncol(green)
green <- green %>% subset(select = -c( VendorID, store_and_fwd_flag))
cat("[", num.vars - ncol(green) , "] variables were delete")
```
## Summary of the data
```{r}
describe(green)
```
## Count NAs' in each column
```{r}
na_count_df(green)
```
## Remove NAs'
As we can see from the output above:
`passenger_count`, `payment_type`, `trip_type`, `congestion_surcharge` have ~ (24 K) NAs'
`fare_amount`, `extra`, `mta_tax`,  `tip_amount`, `tolls_amount`, `improvement_surcharge`, `total_amount` have negative values.
Negative values: we believe have been entered mistakenly as negative values so, we need to fix this problem by obtaining the absolute values.
NAs': We need to drop rows with NAs as they critical in the classification.
As well as, we need to remove `ehail_fee` variable as it's all NAs'.
For `tip_amount` we realized that there are many `0` values, we need to get rid of them.
```{r}
green <- green %>% within(rm(ehail_fee))
num.rows <- nrow(green)
green <- green[!is.na(green$trip_type) & green$trip_distance != 0,]
cat("[", num.rows - nrow(green), "] observations were deleted out of [", num.rows, "]",
    "which means ~ (", round((num.rows - nrow(green))/num.rows, digits = 2), ")")
```
```{r}
as.data.frame(t(data.frame(sapply(green, function(col) sum(is.na(col))))))
```
## Check top & least frequent and negative values
```{r}
describe(green)
```
## Handling `0` values
`passenger_count` variable has some `0` which must be an input error. We can fix this issue by replacing `0` with the rounded mean of all records.
`total_amount` variable has `0` which cannot be handled nor replaced. So, we go with deleting them.
```{r}
avg <- as.integer(round(mean(green$passenger_count), digits = 0))
green$passenger_count <- ifelse(green$passenger_count != 0, green$passenger_count, avg)
green <- green[green$total_amount > 0,]
```
```{r}
str(green)
```
## Convert the negative values to positive by obtaining `abs`
```{r}
green$fare_amount <- abs(green$fare_amount)
green$extra <- abs(green$extra)
green$mta_tax <- abs(green$mta_tax)
green$tip_amount <- abs(green$tip_amount)
green$tolls_amount <- abs(green$tolls_amount)
green$improvement_surcharge <- abs(green$improvement_surcharge)
green$total_amount <- abs(green$total_amount)
```
## Convert categorical variables to factor type
```{r}
green$RatecodeID <- factor(green$RatecodeID)
green$PULocationID <- factor(green$PULocationID)
green$DOLocationID <- factor(green$DOLocationID)
green$payment_type <- factor(green$payment_type)
green$trip_type <- factor(green$trip_type)
```
```{r}
str(green)
```
## Detecting outliers
### Using IQR
After we calculate the outliers using IQR, we inspect each variable to double check and remove manually what we, as exports, believe that it is an outlier.
```{r}
iqr_outliers_min_max(green %>% select_if(is.numeric))
```
```{r}
system.time(iqr.outliers <- green %>% select_if(is.numeric) %>% lapply(iqr_outliers_finder))
```
Now, let us inspect variable values
### passenger_count
```{r}
get_freq_df_from_vector(iqr.outliers$passenger_count, "passenger_count")
```

We can remove observations with `passenger_count > 6` as it is not logical to fit 6 or more people in a taxi.
```{r}
num.obs <- nrow(green)
green <- green[green$passenger_count <= 6, ]
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 2), ")")
```
### trip_distance
```{r}
get_freq_df_from_vector(iqr.outliers$trip_distance, "trip_distance")
```
1. Drop the row with `trip_distance == 134121.5`
2. Take the absolute value of the negative ones
```{r}
green$trip_distance <- abs(green$trip_distance)
green <- green[green$trip_distance < 150, ]

num.obs <- nrow(green)
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 3), ")")
```
### `fare_amount`
```{r}
get_freq_df_from_vector(iqr.outliers$fare_amount, "fare_amount")
```
`753` is an outlier as the trip distance is only `7.49` => drop
`335.5` is an outlier as the trip distance is only `6.97` => drop
Others seem to be accepted
```{r}
green <- green[green$fare_amount %notin% c(753, 335.5),]

num.obs <- nrow(green)
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 3), ")")
```
### `extra`
```{r}
get_freq_df_from_vector(iqr.outliers$extra, "extra")
```
### `mta_tax`
```{r}
get_freq_df_from_vector(iqr.outliers$mta_tax, "mta_tax")
```
### `tip_amount`
```{r}
get_freq_df_from_vector(iqr.outliers$tip_amount, "tip_amount")
```
We remove observations with `tip_amount > 0.7 of the `total_amount`
```{r}
green <- green[(green$tip_amount/green$total_amount) <= 0.7,]

num.obs <- nrow(green)
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 3), ")")
```
### `tolls_amount`
```{r}
get_freq_df_from_vector(iqr.outliers$tolls_amount, "tolls_amount")
```

`96.12`, `48.88`, `35.0` and all values with percentage 0.9 or higher of the `total_amount` are outliers to be removed.
```{r}
green <- green[green$tolls_amount/green$total_amount < 0.5, ]

num.obs <- nrow(green)
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 3), ")")
```
### `improvement_surcharge`
```{r}
get_freq_df_from_vector(iqr.outliers$improvement_surcharge, "improvement_surcharge")
```
### `total_amount`
```{r}
get_freq_df_from_vector(iqr.outliers$total_amount, "total_amount")
```
### `congestion_surcharge`
```{r}
get_freq_df_from_vector(iqr.outliers$congestion_surcharge, "congestion_surcharge")
```
## Save valid data "after dropping outliers"
```{r}
system.time(saveRDS(green, file = "data/valid_data.rds"))
```
## Load valid data
```{r}
system.time(green <- readRDS("data/valid_data.rds"))
```

## Density distribution of  `trip_distance`
```{r}
ggplot(green, aes(x=trip_distance)) +
        geom_histogram(aes(y=..density..), binwidth=.1, colour="black", fill="white") +
        geom_density(alpha=.2, colour="blue", fill="#000066")+  xlim(0, 15)
```

# Classification
## Add `mean` and `median` of `trip_distance` grouped `hour` of pickup time
```{r}
st <- proc.time()
green$pickup_hour <- as.integer(format(strptime(green$lpep_pickup_datetime, "%Y-%m-%d %H:%M:%S"),"%H"))
hourly_trip_distance <- data.frame( green %>%
                                   group_by(pickup_hour) %>%
                                   summarise(mean_trip_dist = mean(trip_distance),
                                             median_trip_dist = median(trip_distance)))
head(hourly_trip_distance)
print(proc.time() - st)
```
```{r}
# Mean trip distance plot
m.trip.dist.plt <- ggplot(hourly_trip_distance, aes(x=pickup_hour, y=mean_trip_dist)) + geom_bar(stat = "identity")
# Median trip distance plot
M.trip.dist.plt <- ggplot(hourly_trip_distance, aes(x=pickup_hour, y=median_trip_dist)) + geom_bar(stat = "identity")
grid.arrange(m.trip.dist.plt, M.trip.dist.plt, ncol=1, nrow =2)
```
From above, we conclude that the longest trips are at `5` & `6` in the morning. In rest of the day's hours trips are somehow close in terms of distance traveled.
//////////////////////////////////////////////////////////////////////////

## `tip_percentage` is the tip percentage based on `total_amount` 
```{r}
st <- proc.time()
green$tip_percentage <- ifelse(green$tip_amount==0.0 | green$total_amount==0.0 , 0.0,
                        round(green$tip_amount/green$total_amount,4))
# Remove all zero percentage as they are not going to help us in classification
num.obs <- nrow(green)
green <- green[green$tip_percentage > 0,]
cat("[", num.obs-nrow(green), "] Observation have been deleted! Which is ~ (", round((num.obs-nrow(green))/num.obs, digits = 3), ")")
print(proc.time() - st)

ggplot(green, aes(x=total_amount, y=tip_amount)) +
    geom_point(
        color="black",
        fill="#69b3a2",
        shape=22,
        alpha=0.3,
        size=3,
        stroke =2
        ) +
    theme_ipsum()
```
`geom_point` is useful when we want to compare two continuous variables.
//////////////////////////////////////////////////////////////////////////

```{r}
cat("Average tip percentage of the total amount ~ (",
    round((sum(green$tip_amount)/sum(green$total_amount)),4)*100 ," %)")
```
## Histogram of `tip_percentage`
```{r}
hist(green$tip_percentage)
```
## Classification using Decision Tree
```{r}
st <- proc.time()
sample.size <- floor(0.75*nrow(green))
sample <- sample(seq_len(nrow(green)), sample.size)
train.set <- green[sample, ]
test.set <- green[-sample, ]

model <- rpartXse(tip_percentage ~ ., train.set, se = 0.5)
predicted <- predict(model, test.set)
print(proc.time() - st)
head(predicted)
```

```{r}
#table(tst_set$tip_percentage, predicted)

```

```{r}

errorRate <- sum(predicted != test.set$tip_percentage) / nrow(test.set)
errorRate
```

```{r}
tr <- train.set
ts <- test.set
s <- svm(tip_percentage ~ ., tr)
ps <- predict(s, ts)
cm <- table(ps, ts$tip_percentage)
```


```{r}
train = train.set
test = test.set

model_reg = svm(tip_percentage~., data=train)
print(model_reg)

```

```{r}
pred = predict(model_reg, test)
 
x = 1:length(test$tip_percentage)
plot(x, test$tip_percentage, pch=18, col="red")
lines(x, pred, lwd="1", col="blue")

```

```{r}

mse = mse(test$tip_percentage, pred)
mae = mae(test$tip_percentage, pred)
rmse = rmse(test$tip_percentage, pred)
r2 = R2(test$tip_percentage, pred, form = "traditional")
 
cat(" MAE:", mae, "\n", "MSE:", mse, "\n", 
     "RMSE:", rmse, "\n", "R-squared:", r2)
```

```{r}
head(test$tip_percentage, n=5)
head(pred, n=5)
```

```{r}

tr <- train.set
ts <- test.set

tr$PULocationID <- NULL
ts$PULocationID <- NULL
tr$DOLocationID <- NULL
ts$DOLocationID <- NULL

nr <- nnet(tip_percentage ~ ., tr,
           linout=TRUE,
           trace=FALSE,
           size=6,
           decay=0.01,
           maxit=2000)
psnr <- predict(nr, ts)
mean(abs(psnr-ts$tip_percentage))
```

```{r}
plot(ts$tip_percentage, psnr)
abline(0, 1)
```

```{r}
h2oInstance <- h2o.init(ip = "localhost") # start H2O instance locally

```

```{r}
trH <- as.h2o(tr,"trH")
tsH <- as.h2o(ts,"tsH")
mdl <- h2o.deeplearning(x=1:18, y=19, training_frame=trH,
hidden = c(100, 100, 100, 100, 100, 100, 100), epochs = 1000)
preds <- as.vector(h2o.predict(mdl,tsH))
```

```{r}
mean(abs(preds - as.vector(tsH$tip_percentage)))
```

```{r}
plot(as.vector(tsH$tip_percentage), preds)
abline(0, 1)
```

```{r}
plot(as.vector(tsH$tip_percentage), preds)
points(as.vector(tsH$tip_percentage), psnr, col = "red")
abline(0, 1)
```

```{r}
h2o.shutdown(prompt = F);
```



```{r}
mars1 <- earth(
tip_percentage ~ .,
data = tr
)

```
```{r}
print(mars1)
```

```{r}
summary(mars1) %>% .$coefficients #%>% head(10)
```

```{r}
plot(mars1, which = 1)
```


```{r}
optimal_tree <- rpart(
formula = tip_percentage ~ .,
data = tr,
method = "anova",
control = list(minsplit = 11, maxdepth = 8, cp = 0.01)
)
plotcp(optimal_tree)
```
